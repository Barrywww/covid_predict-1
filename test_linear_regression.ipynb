{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the .csv files within one folder\n",
    "# return -> dict: {key=csv filename, value=pd.DataFrame}\n",
    "def load_all_files(path):\n",
    "    d = {}\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            d[file.split(\".\")[0]] = pd.read_csv(os.path.join(root, file), index_col=0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_all_files(\"./preprocessed_data_Barry\")\n",
    "data_dict_t = load_all_files(\"./preprocessed_data_Tya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['new_cases', 'new_cases_per_million', 'new_deaths', 'new_deaths_per_million', 'new_tests', 'positive_rate', 'stringency_index', 'total_cases', 'total_deaths', 'total_deaths_per_million', 'total_tests'])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, df in data_dict_t.items():\n",
    "    data_dict_t[k] = df.iloc[:-1,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[\"new_cases\"] = data_dict[\"new_cases\"].iloc[:-1, 1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features & country data to be included into the model\n",
    "selected_features = ['C1_School_closing', 'C2_Workplace_closing', 'C3_Cancel_public_events', 'C4_Restrictions_on_gatherings',\n",
    "                     'C5_Close_public_transport', 'C6_Stay_at_home_requirements', 'C7_Restrictions_on_internal_movement', \n",
    "                     'C8_International_travel_controls', 'E1_Income_support', 'E2_Debt_contract_relief', \n",
    "                     'E3_Fiscal_measures', 'E4_International_support', 'H1_Public_information_campaigns', 'H2_Testing_policy', 'H3_Contact_tracing',\n",
    "                     'H4_Emergency_investment_in_healthcare', 'H5_Investment_in_vaccines', 'H6_Facial_Coverings', 'I_ContainmentHealthIndex', \n",
    "                     'I_GovernmentResponseIndex', 'I_StringencyIndex', 'new_cases']\n",
    "\n",
    "selected_features_t = ['new_cases_per_million', 'new_deaths', 'new_deaths_per_million', \n",
    "                       'new_tests', 'positive_rate', 'stringency_index', 'total_deaths', \n",
    "                       'total_deaths_per_million', 'total_tests', 'new_cases']\n",
    "\n",
    "# selected_country = [\"ARG\"]\n",
    "# selected_country_t = [\"JPN\"]\n",
    "selected_country = list(data_dict['C1_School_closing'].index)\n",
    "selected_country_t = list(data_dict_t['total_cases'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a date * feature sample dataframe\n",
    "dates = data_dict[\"ConfirmedCases\"].columns[1:]\n",
    "sample_df = pd.DataFrame(columns=selected_features, index=dates, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_t = data_dict_t[\"total_cases\"].columns[1:]\n",
    "sample_df_t = pd.DataFrame(columns=selected_features_t, index=dates, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_cases_per_million</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_per_million</th>\n",
       "      <th>new_tests</th>\n",
       "      <th>positive_rate</th>\n",
       "      <th>stringency_index</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>total_tests</th>\n",
       "      <th>new_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            new_cases_per_million  new_deaths  new_deaths_per_million  \\\n",
       "2020-01-02                    NaN         NaN                     NaN   \n",
       "2020-01-03                    NaN         NaN                     NaN   \n",
       "2020-01-04                    NaN         NaN                     NaN   \n",
       "2020-01-05                    NaN         NaN                     NaN   \n",
       "2020-01-06                    NaN         NaN                     NaN   \n",
       "...                           ...         ...                     ...   \n",
       "2020-11-27                    NaN         NaN                     NaN   \n",
       "2020-11-28                    NaN         NaN                     NaN   \n",
       "2020-11-29                    NaN         NaN                     NaN   \n",
       "2020-11-30                    NaN         NaN                     NaN   \n",
       "2020-12-01                    NaN         NaN                     NaN   \n",
       "\n",
       "            new_tests  positive_rate  stringency_index  total_deaths  \\\n",
       "2020-01-02        NaN            NaN               NaN           NaN   \n",
       "2020-01-03        NaN            NaN               NaN           NaN   \n",
       "2020-01-04        NaN            NaN               NaN           NaN   \n",
       "2020-01-05        NaN            NaN               NaN           NaN   \n",
       "2020-01-06        NaN            NaN               NaN           NaN   \n",
       "...               ...            ...               ...           ...   \n",
       "2020-11-27        NaN            NaN               NaN           NaN   \n",
       "2020-11-28        NaN            NaN               NaN           NaN   \n",
       "2020-11-29        NaN            NaN               NaN           NaN   \n",
       "2020-11-30        NaN            NaN               NaN           NaN   \n",
       "2020-12-01        NaN            NaN               NaN           NaN   \n",
       "\n",
       "            total_deaths_per_million  total_tests  new_cases  \n",
       "2020-01-02                       NaN          NaN        NaN  \n",
       "2020-01-03                       NaN          NaN        NaN  \n",
       "2020-01-04                       NaN          NaN        NaN  \n",
       "2020-01-05                       NaN          NaN        NaN  \n",
       "2020-01-06                       NaN          NaN        NaN  \n",
       "...                              ...          ...        ...  \n",
       "2020-11-27                       NaN          NaN        NaN  \n",
       "2020-11-28                       NaN          NaN        NaN  \n",
       "2020-11-29                       NaN          NaN        NaN  \n",
       "2020-11-30                       NaN          NaN        NaN  \n",
       "2020-12-01                       NaN          NaN        NaN  \n",
       "\n",
       "[335 rows x 10 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_scale(df, threshold=10, factor=100):\n",
    "    for col in df.columns:\n",
    "        if df[col].mean() < threshold:\n",
    "            df[col] = df[col] * factor * random.randint(1,11)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_yesterday(df):\n",
    "    yesterday = [0]\n",
    "    yesterday.extend(list(df[\"new_cases\"]))\n",
    "    df[\"YesterdayConfirmed\"] =yesterday[:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_regression(country, concat_df, standarize=True, scale=False, verbose=False):\n",
    "    # insert corresponding feature data from dataframes\n",
    "    for feature in selected_features:\n",
    "        concat_df[feature] = data_dict[feature].loc[country]\n",
    "            \n",
    "     # clear out invalid tail data       \n",
    "    concat_df = concat_df.iloc[:-25,:].copy()\n",
    "    \n",
    "    # insert \"YesterdayConfirmed\" feature\n",
    "    concat_df = insert_yesterday(concat_df).copy()\n",
    "    \n",
    "    # replace NaN with 0\n",
    "    concat_df = concat_df.fillna(0)\n",
    "    if scale:\n",
    "        concat_df = do_scale(concat_df, 10, 10000).copy()\n",
    "    \n",
    "    # create train&test data\n",
    "    X = np.array(concat_df[selected_features[:-1]+ [\"YesterdayConfirmed\"]])\n",
    "    y = np.array(concat_df[selected_features[-1]]).reshape(-1,1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # whether to scale the data, True by default\n",
    "    if standarize:\n",
    "        scaler_x = StandardScaler()\n",
    "        X_train = scaler_x.fit_transform(X_train)\n",
    "        X_test  = scaler_x.transform(X_test)\n",
    "        X_train = np.hstack((np.ones(X_train.shape[0]).reshape(-1,1), X_train))\n",
    "        X_test = np.hstack((np.ones(X_test.shape[0]).reshape(-1,1), X_test))\n",
    "\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train = scaler_y.fit_transform(y_train)\n",
    "        y_test  = scaler_y.transform(y_test)\n",
    "    \n",
    "    # create linear regression model & train\n",
    "#     regr = linear_model.LinearRegression()\n",
    "    regr = Ridge(alpha=1.5)\n",
    "#     regr = Lasso(alpha=0.001)\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    if verbose:\n",
    "        # print out statistical data\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Country:\", country)\n",
    "        print('Coefficients: \\n')\n",
    "    #     print(regr.coef_)\n",
    "        for i in range(0, len(regr.coef_[0]) -1):\n",
    "            print(\"Feature: %s\" % selected_features[i])\n",
    "            print(\"Weight: %.6f\" % regr.coef_[0][i + 1])\n",
    "    #         print()\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "        plt.semilogy(y_pred, label = \"y_pred\", color=\"red\");\n",
    "        plt.semilogy(y_test, label = \"y_test\", color=\"blue\");\n",
    "\n",
    "        plt.show()\n",
    "    return regr.coef_[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_regression_t(country, concat_df, standarize=True, scale=False, verbose=False):\n",
    "    # insert corresponding feature data from dataframes\n",
    "    for feature in selected_features_t:\n",
    "        concat_df[feature] = data_dict_t[feature].loc[country]\n",
    "            \n",
    "     # clear out invalid tail data       \n",
    "    concat_df = concat_df.iloc[:-50,:].copy()\n",
    "    \n",
    "    # insert \"YesterdayConfirmed\" feature\n",
    "    concat_df = insert_yesterday(concat_df).copy()\n",
    "    \n",
    "    # replace NaN with 0\n",
    "    concat_df = concat_df.fillna(0)\n",
    "    if scale:\n",
    "        concat_df = do_scale(concat_df, 10, 10000).copy()\n",
    "    \n",
    "    # create train&test data\n",
    "    X = np.array(concat_df[selected_features_t[:-1]+ [\"YesterdayConfirmed\"]])\n",
    "    y = np.array(concat_df[selected_features_t[-1]]).reshape(-1,1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # whether to scale the data, True by default\n",
    "    if standarize:\n",
    "        scaler_x = StandardScaler()\n",
    "        X_train = scaler_x.fit_transform(X_train)\n",
    "        X_test  = scaler_x.transform(X_test)\n",
    "        X_train = np.hstack((np.ones(X_train.shape[0]).reshape(-1,1), X_train))\n",
    "        X_test = np.hstack((np.ones(X_test.shape[0]).reshape(-1,1), X_test))\n",
    "\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train = scaler_y.fit_transform(y_train)\n",
    "        y_test  = scaler_y.transform(y_test)\n",
    "    \n",
    "    # create linear regression model & train\n",
    "#     regr = linear_model.LinearRegression()\n",
    "    regr = Ridge(alpha=1.5)\n",
    "#     regr = Lasso(alpha=0.001)\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    if verbose:\n",
    "        # print out statistical data\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"Country:\", country)\n",
    "        print('Coefficients: \\n')\n",
    "    #     print(regr.coef_)\n",
    "        for i in range(0, len(regr.coef_[0]) -1):\n",
    "            print(\"Feature: %s\" % selected_features_t[i])\n",
    "            print(\"Weight: %.6f\" % regr.coef_[0][i + 1])\n",
    "    #         print()\n",
    "        print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "        print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "        plt.semilogy(y_pred, label = \"y_pred\", color=\"red\");\n",
    "        plt.semilogy(y_test, label = \"y_test\", color=\"blue\");\n",
    "\n",
    "        plt.show()\n",
    "    return regr.coef_[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepare statistical dict for coefs\n",
    "coefs = {}\n",
    "for k in data_dict.keys():\n",
    "    coefs[k] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating: ABW 0\n",
      "Calculating: AFG 1\n",
      "Calculating: AGO 2\n",
      "Calculating: ALB 3\n",
      "Calculating: AND 4\n",
      "Calculating: ARE 5\n",
      "Calculating: ARG 6\n",
      "Calculating: AUS 7\n",
      "Calculating: AUT 8\n",
      "Calculating: AZE 9\n",
      "Calculating: BDI 10\n",
      "Calculating: BEL 11\n",
      "Calculating: BEN 12\n",
      "Calculating: BFA 13\n",
      "Calculating: BGD 14\n",
      "Calculating: BGR 15\n",
      "Calculating: BHR 16\n",
      "Calculating: BHS 17\n",
      "Calculating: BIH 18\n",
      "Calculating: BLR 19\n",
      "Calculating: BLZ 20\n",
      "Calculating: BMU 21\n",
      "Calculating: BOL 22\n",
      "Calculating: BRA 23\n",
      "Calculating: BRB 24\n",
      "Calculating: BRN 25\n",
      "Calculating: BTN 26\n",
      "Calculating: BWA 27\n",
      "Calculating: CAF 28\n",
      "Calculating: CAN 29\n",
      "Calculating: CHE 30\n",
      "Calculating: CHL 31\n",
      "Calculating: CHN 32\n",
      "Calculating: CIV 33\n",
      "Calculating: CMR 34\n",
      "Calculating: COD 35\n",
      "Calculating: COG 36\n",
      "Calculating: COL 37\n",
      "Calculating: COM 38\n",
      "Calculating: CPV 39\n",
      "Calculating: CRI 40\n",
      "Calculating: CUB 41\n",
      "Calculating: CYP 42\n",
      "Calculating: CZE 43\n",
      "Calculating: DEU 44\n",
      "Calculating: DJI 45\n",
      "Calculating: DMA 46\n",
      "Calculating: DNK 47\n",
      "Calculating: DOM 48\n",
      "Calculating: DZA 49\n",
      "Calculating: ECU 50\n",
      "Calculating: EGY 51\n",
      "Calculating: ERI 52\n",
      "Calculating: ESP 53\n",
      "Calculating: EST 54\n",
      "Calculating: ETH 55\n",
      "Calculating: FIN 56\n",
      "Calculating: FJI 57\n",
      "Calculating: FRA 58\n",
      "Calculating: FRO 59\n",
      "Calculating: GAB 60\n",
      "Calculating: GBR 61\n",
      "Calculating: GEO 62\n",
      "Calculating: GHA 63\n",
      "Calculating: GIN 64\n",
      "Calculating: GMB 65\n",
      "Calculating: GRC 66\n",
      "Calculating: GRL 67\n",
      "Calculating: GTM 68\n",
      "Calculating: GUM 69\n",
      "Calculating: GUY 70\n",
      "Calculating: HKG 71\n",
      "Calculating: HND 72\n",
      "Calculating: HRV 73\n",
      "Calculating: HTI 74\n",
      "Calculating: HUN 75\n",
      "Calculating: IDN 76\n",
      "Calculating: IND 77\n",
      "Calculating: IRL 78\n",
      "Calculating: IRN 79\n",
      "Calculating: IRQ 80\n",
      "Calculating: ISL 81\n",
      "Calculating: ISR 82\n",
      "Calculating: ITA 83\n",
      "Calculating: JAM 84\n",
      "Calculating: JOR 85\n",
      "Calculating: JPN 86\n",
      "Calculating: KAZ 87\n",
      "Calculating: KEN 88\n",
      "Calculating: KGZ 89\n",
      "Calculating: KHM 90\n",
      "Calculating: KOR 91\n",
      "Calculating: KWT 92\n",
      "Calculating: LAO 93\n",
      "Calculating: LBN 94\n",
      "Calculating: LBR 95\n",
      "Calculating: LBY 96\n",
      "Calculating: LKA 97\n",
      "Calculating: LSO 98\n",
      "Calculating: LTU 99\n",
      "Calculating: LUX 100\n",
      "Calculating: LVA 101\n",
      "Calculating: MAC 102\n",
      "Calculating: MAR 102\n",
      "Calculating: MCO 103\n",
      "Calculating: MDA 104\n",
      "Calculating: MDG 105\n",
      "Calculating: MEX 106\n",
      "Calculating: MLI 107\n",
      "Calculating: MMR 108\n",
      "Calculating: MNG 109\n",
      "Calculating: MOZ 110\n",
      "Calculating: MRT 111\n",
      "Calculating: MUS 112\n",
      "Calculating: MWI 113\n",
      "Calculating: MYS 114\n",
      "Calculating: NAM 115\n",
      "Calculating: NER 116\n",
      "Calculating: NGA 117\n",
      "Calculating: NIC 118\n",
      "Calculating: NLD 119\n",
      "Calculating: NOR 120\n",
      "Calculating: NPL 121\n",
      "Calculating: NZL 122\n",
      "Calculating: OMN 123\n",
      "Calculating: PAK 124\n",
      "Calculating: PAN 125\n",
      "Calculating: PER 126\n",
      "Calculating: PHL 127\n",
      "Calculating: PNG 128\n",
      "Calculating: POL 129\n",
      "Calculating: PRI 130\n",
      "Calculating: PRT 131\n",
      "Calculating: PRY 132\n",
      "Calculating: PSE 133\n",
      "Calculating: QAT 134\n",
      "Calculating: RKS 135\n",
      "Calculating: ROU 135\n",
      "Calculating: RUS 136\n",
      "Calculating: RWA 137\n",
      "Calculating: SAU 138\n",
      "Calculating: SDN 139\n",
      "Calculating: SEN 140\n",
      "Calculating: SGP 141\n",
      "Calculating: SLB 142\n",
      "Calculating: SLE 143\n",
      "Calculating: SLV 144\n",
      "Calculating: SMR 145\n",
      "Calculating: SOM 146\n",
      "Calculating: SRB 147\n",
      "Calculating: SSD 148\n",
      "Calculating: SUR 149\n",
      "Calculating: SVK 150\n",
      "Calculating: SVN 151\n",
      "Calculating: SWE 152\n",
      "Calculating: SWZ 153\n",
      "Calculating: SYC 154\n",
      "Calculating: SYR 155\n",
      "Calculating: TCD 156\n",
      "Calculating: TGO 157\n",
      "Calculating: THA 158\n",
      "Calculating: TJK 159\n",
      "Calculating: TKM 160\n",
      "Calculating: TLS 160\n",
      "Calculating: TTO 161\n",
      "Calculating: TUN 162\n",
      "Calculating: TUR 163\n",
      "Calculating: TWN 164\n",
      "Calculating: TZA 165\n",
      "Calculating: UGA 166\n",
      "Calculating: UKR 167\n",
      "Calculating: URY 168\n",
      "Calculating: USA 169\n",
      "Calculating: UZB 170\n",
      "Calculating: VEN 171\n",
      "Calculating: VIR 172\n",
      "Calculating: VNM 173\n",
      "Calculating: VUT 174\n",
      "Calculating: YEM 174\n",
      "Calculating: ZAF 175\n",
      "Calculating: ZMB 176\n",
      "Calculating: ZWE 177\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "verbose=False\n",
    "for country in selected_country:\n",
    "    print(\"Calculating:\", country, idx)\n",
    "    try:\n",
    "        c = do_regression(country, sample_df, verbose=verbose)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if not verbose:\n",
    "        if (max(c) >= 5 or min(c) <= -5):\n",
    "            print(\"WARNING:\", country, c)\n",
    "        else:\n",
    "            for k in range(len(c)):\n",
    "                coefs[selected_features[k]].append(c[k])\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1_School_closing 0.00295\n",
      "C2_Workplace_closing 0.05707\n",
      "C3_Cancel_public_events 0.02533\n",
      "C4_Restrictions_on_gatherings 0.00476\n",
      "C5_Close_public_transport 0.00355\n",
      "C6_Stay_at_home_requirements 0.02335\n",
      "C7_Restrictions_on_internal_movement 0.02492\n",
      "C8_International_travel_controls 0.02051\n",
      "E1_Income_support 0.02933\n",
      "E2_Debt_contract_relief 0.02515\n",
      "E3_Fiscal_measures -0.00796\n",
      "E4_International_support 0.00205\n",
      "H1_Public_information_campaigns 0.01166\n",
      "H2_Testing_policy -0.00354\n",
      "H3_Contact_tracing -0.00679\n",
      "H4_Emergency_investment_in_healthcare 0.0095\n",
      "H5_Investment_in_vaccines 0.00068\n",
      "H6_Facial_Coverings 0.09785\n",
      "I_ContainmentHealthIndex -0.00616\n",
      "I_GovernmentResponseIndex 0.00598\n",
      "I_StringencyIndex -0.01395\n",
      "new_cases 0.31073\n"
     ]
    }
   ],
   "source": [
    "for f in selected_features:\n",
    "    print(f, round(np.array(coefs[f]).mean(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
